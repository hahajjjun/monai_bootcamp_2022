{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96ohkCyqZNaM"
   },
   "source": [
    "#  MONAI Bootcamp\n",
    "## Domain shift regression\n",
    "\n",
    "<img src=\"https://github.com/Project-MONAI/MONAIBootcamp2021/raw/2f28b64f814a03703667c8ea18cc84f53d6795e4/day1/monai.png\" width=400>\n",
    "\n",
    "This notebook is a brief template to generate synthetic gadolinium-enhanced T1w brain image from FLAIR(Fluid Attenuated Inversion Recovery), T1w and T2w images.\n",
    "\n",
    "For the sake of speed, a 2D dataset has been created by taking slices from the 3D BRATS brain tumor dataset.\n",
    "\n",
    "The dataset comes from http://medicaldecathlon.com/.\n",
    "\n",
    "A number of blanks need to be filled in to get some results, and then improvements can be made to improve upon these!\n",
    "\n",
    "#### Required Packages \n",
    "The servers running MONAI Bootcamp already have CUDA driver, CUDA toolkit 11.0, pytorch 1.6+, monai 0.8.1, and libraries for practice installed.\n",
    "\n",
    "This notebook has the pip command for installing MONAI and will be added to any subsequent notebook in BYOD or colab environment.\n",
    "\n",
    "Execute the following cell to install MONAI the first time a colab notebook or your own environment is run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1aLtpMbJVNCH",
    "outputId": "d51919a6-53b5-4f7f-9e05-dc4afac2bbf0"
   },
   "outputs": [],
   "source": [
    "#!pip install -qU \"monai[ignite, nibabel, torchvision, tqdm]==0.8.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p7pcbsphZh8G"
   },
   "source": [
    "### Check GPU \n",
    " Running  `!nvidia-smi` in a cell will verify this has worked and show you what kind of hardware you have access to.\n",
    " if GPU Memory Usage is no `0 MiB` shutdown all kernels and restart current kernel.\n",
    "- step1. shutdown kernel with following <b>Menu</b> > <b>Kernel</b> > <b>Shut Down All kernels </b>\n",
    "- step2. restart kernelw with following <b>Menu</b> > <b>Kernel</b> > <b>Restart Kernel</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZVD7911EVcWI",
    "outputId": "12cc6eb6-2b9a-4131-b4c3-8b4d305a023a"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aX4DkxZygKNP"
   },
   "source": [
    "### Setup imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pNnKTuzBgKZm",
    "outputId": "05be7817-8173-4a3d-b591-eec56bccf845"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm import trange\n",
    "import os\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from monai.apps import download_and_extract\n",
    "from monai.config import print_config\n",
    "from monai.data import Dataset, DataLoader, partition_dataset\n",
    "from monai.networks import eval_mode\n",
    "from monai.transforms import (\n",
    "    Compose,\n",
    "    EnsureTyped,\n",
    "    LoadImaged,\n",
    "    MapTransform,\n",
    "    rescale_array,\n",
    "    ScaleIntensityd,\n",
    ")\n",
    "from monai.utils import set_determinism\n",
    "\n",
    "print_config()\n",
    "set_determinism(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting up our Dataset and exploring the data\n",
    "#### Setup data directory\n",
    "\n",
    "We'll create a temporary directory for all the MONAI data we're going to be using called temp directory in `~/monai-lab/temp`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "em9KLARygwYg"
   },
   "outputs": [],
   "source": [
    "import os \n",
    "directory = \"temp\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "root_dir = tempfile.mkdtemp() if directory is None else directory\n",
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resource = \"https://drive.google.com/uc?id=17f4J_rU5pi1zRmxMe5OwljyT3tlBf6qI&confirm=no_antivirus\"\n",
    "data_dir = os.path.join(root_dir, \"brain_2d\")\n",
    "compressed_file = os.path.join(root_dir, \"brain_2d.tar.gz\")\n",
    "print(data_dir)\n",
    "if not os.path.exists(data_dir):\n",
    "    download_and_extract(resource, compressed_file, root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nRQ9g2N2mxfb"
   },
   "source": [
    "### visualize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xLNOXVT_ksdq",
    "outputId": "15852697-7c60-43ec-b3af-09d7ed402477"
   },
   "outputs": [],
   "source": [
    "input_ims = sorted(glob(os.path.join(data_dir, \"*input.npy\")))\n",
    "output_ims = sorted(glob(os.path.join(data_dir, \"*GT_output.npy\")))\n",
    "data = [{\"input\": i, \"output\": o} for i, o in zip(input_ims, output_ims)]\n",
    "print(\"number data points\", len(data))\n",
    "print(\"example\", data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QPtP1GLHoCtP"
   },
   "outputs": [],
   "source": [
    "data_sample = np.load('temp/brain_2d/0_input.npy')\n",
    "gt_sample = np.load('temp/brain_2d/0_GT_output.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CoMBmJUKphig",
    "outputId": "9feb0288-b387-4069-de88-4caa693b4506"
   },
   "outputs": [],
   "source": [
    "print(\"data shape:\", data_sample.shape, gt_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 342
    },
    "id": "xAkkY42gpkFB",
    "outputId": "40660836-5f95-460c-f94e-9799f292d771"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4 ,  sharex=True, sharey=True, figsize=(24,6))\n",
    "\n",
    "ax1.imshow(data_sample[0], cmap='gray' )\n",
    "ax1.set_title('FLAIR')\n",
    "ax2.imshow(data_sample[1], cmap='gray' )\n",
    "ax2.set_title('T1w')\n",
    "ax3.imshow(data_sample[2], cmap='gray' )\n",
    "ax3.set_title('T2w')\n",
    "ax4.imshow(gt_sample[0], cmap='gray' )\n",
    "ax4.set_title('GT')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hX0qv6pOqU7W"
   },
   "source": [
    "## split dataset( train / valid 20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelWiseScaleIntensityd(MapTransform):\n",
    "    \"\"\"Perform channel-wise intensity normalisation.\"\"\"\n",
    "    def __init__(self, keys):\n",
    "        super().__init__(keys)\n",
    "    def __call__(self, d):\n",
    "        for key in self.keys:\n",
    "            for idx, channel in enumerate(d[key]):\n",
    "                d[key][idx] = rescale_array(channel)\n",
    "        return d\n",
    "\n",
    "keys = [\"input\", \"output\"]\n",
    "train_transforms = Compose([\n",
    "    LoadImaged(keys),\n",
    "    ChannelWiseScaleIntensityd(\"input\"),\n",
    "    ScaleIntensityd(\"output\"),\n",
    "    EnsureTyped(keys),\n",
    "])\n",
    "val_transforms = Compose([\n",
    "    LoadImaged(keys),\n",
    "    ChannelWiseScaleIntensityd(\"input\"),\n",
    "    ScaleIntensityd(\"output\"),\n",
    "    EnsureTyped(keys),\n",
    "])\n",
    "\n",
    "t = train_transforms(data[0])\n",
    "print(t[\"input\"].shape, t[\"output\"].shape)\n",
    "in_channels, out_channels = t[\"input\"].shape[0], t[\"output\"].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gTRSGFYFoCwV",
    "outputId": "32fa0aa5-a437-433d-b019-82709bd8ce51"
   },
   "outputs": [],
   "source": [
    "# split data into 80% and 20% for training and validation, respectively\n",
    "train_data, val_data = partition_dataset(data, (8, 2), shuffle=True)\n",
    "print(\"num train data points:\", len(train_data))\n",
    "print(\"num val data points:\", len(val_data))\n",
    "batch_size = 40 ##### default 10 1GB, AE : 40 2GiB\n",
    "num_workers = 4 ###### default 10 \n",
    "train_ds = Dataset(train_data, train_transforms)\n",
    "train_dl = DataLoader(train_ds, num_workers=num_workers, batch_size=batch_size, shuffle=True)\n",
    "val_ds = Dataset(val_data, val_transforms)\n",
    "val_dl = DataLoader(val_ds, num_workers=num_workers, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iDaddax3mPug"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def imshows(ims):\n",
    "    \"\"\"Visualises a list of dictionaries.\n",
    "\n",
    "    Each key of the dictionary will be used as a column, and\n",
    "    each element of the list will be a row.\n",
    "    \"\"\"\n",
    "    nrow = len(ims)\n",
    "    ncol = len(ims[0])\n",
    "    fig, axes = plt.subplots(nrow, ncol, figsize=(\n",
    "        ncol * 4, nrow * 4), facecolor='white')\n",
    "    for i, im_dict in enumerate(ims):\n",
    "        for j, (title, im) in enumerate(im_dict.items()):\n",
    "            if isinstance(im, torch.Tensor):\n",
    "                im = im.detach().cpu().numpy()\n",
    "            # If RGB, put to end. Else, average across channel dim\n",
    "            if im.ndim > 2:\n",
    "                im = np.moveaxis(im, 0, -1) if im.shape[0] == 3 else np.mean(im, axis=0)\n",
    "\n",
    "            ax = axes[j] if len(ims) == 1 else axes[i, j]\n",
    "            ax.set_title(f\"{title}\\n{im.shape}\")\n",
    "            im_show = ax.imshow(im, cmap='gray')\n",
    "            ax.axis(\"off\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "8yt2oxEMloFS",
    "outputId": "34f93087-93ff-4011-a464-e6eadd5a232e"
   },
   "outputs": [],
   "source": [
    "to_imshow = []\n",
    "for idx in np.random.choice(len(val_ds), size=5, replace=False):\n",
    "    rand_data = val_ds[idx]\n",
    "    rand_input, rand_output_gt = rand_data[\"input\"], rand_data[\"output\"]\n",
    "\n",
    "    to_imshow.append(\n",
    "        {\n",
    "            \"FLAIR\": rand_input[0],\n",
    "            \"T1w\": rand_input[1],\n",
    "            \"T2w\": rand_input[2],\n",
    "            \"GT GD\": rand_output_gt,\n",
    "\n",
    "        }\n",
    "    )\n",
    "imshows(to_imshow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2CE8TA38JQuP"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Template for train\n",
    "\n",
    "#### configure model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "48OGFcZwk1_K"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Create loss fn and optimiser\n",
    "\n",
    "model = None  # TODO\n",
    "\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train script script "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 10  #None  # TODO\n",
    "loss_function = None  # TODO\n",
    "optimizer = None  # TODO\n",
    "epoch_losses = []\n",
    "\n",
    "t = trange(max_epochs, desc=f\"epoch 0, avg loss: inf\", leave=True)\n",
    "for epoch in t:\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    for batch in train_dl:\n",
    "        step += 1\n",
    "        inputs, outputs_gt = batch[\"input\"].to(device), batch[\"output\"].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, outputs_gt)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    epoch_loss /= step\n",
    "    epoch_losses.append(epoch_loss)\n",
    "    t.set_description(f\"epoch {epoch + 1}, average loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "obSCDT0F05zt"
   },
   "source": [
    "### Monai Network\n",
    "MONAI provides predefined networks. we can easily import it. \n",
    "\n",
    "- [Layers](https://docs.monai.io/en/stable/networks.html#layers) : Act, Conv, Norm, Dropout, Flatten, Reshape, Pad, Pool, SkipConnection\n",
    "- [Blocks](https://docs.monai.io/en/stable/networks.html#module-monai.networks.blocks) : ADN, Convolution, Synamic UnetBlock, FCN, GCN, Squeeze-andExcitation, ResNeXt, SABlock, Transformer Block, \n",
    "- [Nets](https://docs.monai.io/en/stable/networks.html#nets)  : DenseNet121, EfficientNet, SegResNet, ResNet, SENet154, DyUNet, UNet, AutoEncoder, VarAutoEncoder, ViT, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1. AutoEncoder\n",
    "\n",
    "<img align='left' src=\"https://miro.medium.com/max/1400/1*nGFy96r63GwSE_EsJDLMDw.png\" width=600>   - 4 layers with  `[4, 8, 16, 32]` channels \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "image from [medium blog ](https://medium.com/geekculture/variational-autoencoder-vae-9b8ce5475f68)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oSAlyRvPy-S5"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "from monai.networks.nets import AutoEncoder\n",
    "\n",
    "model = AutoEncoder(\n",
    "    spatial_dims =2,\n",
    "    in_channels=in_channels,\n",
    "    out_channels=out_channels,\n",
    "    channels=(4, 8, 16, 32),\n",
    "    strides=(2, 2, 2, 2),\n",
    ").to(device)  # TODO\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WvRTTMkUz_8k",
    "outputId": "c9651700-a7f8-46a0-f567-627bf1ac0ed2"
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create loss fn and optimiser\n",
    "\n",
    "\n",
    "loss_function = torch.nn.MSELoss() # TODO\n",
    "\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.Adam(model.parameters(), learning_rate)  # TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 20  #None  # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will take more 10 minutes to finish 20 epoch training. it depends on system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xKXT4ik8zkN-",
    "outputId": "b2666112-c625-4f2c-9e35-b487e8c1ac64"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "epoch_losses = []\n",
    "\n",
    "t = trange(max_epochs, desc=f\"epoch 0, avg loss: inf\", leave=True)\n",
    "for epoch in t:\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    for batch in train_dl:\n",
    "        step += 1\n",
    "        inputs, outputs_gt = batch[\"input\"].to(device), batch[\"output\"].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, outputs_gt)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    epoch_loss /= step\n",
    "    epoch_losses.append(epoch_loss)\n",
    "    t.set_description(f\"epoch {epoch + 1}, average loss: {epoch_loss:.4f}\")\n",
    "    if (epoch+1) % 10 == 0: \n",
    "        torch.save(model.state_dict(), os.path.join(root_dir, \"gan_ae1_model_{:04d}.pth\".format(epoch+1)))\n",
    "        torch.save(optimizer.state_dict(), os.path.join(root_dir, \"gan_ae1_optim_{:04d}.pth\".format(epoch+1)))\n",
    "        #print(\"|{}ep model saved\".format(epoch+1), end='')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "wYxoyVmxlLSo",
    "outputId": "61841b2a-085e-4838-91ad-e7944e746153"
   },
   "outputs": [],
   "source": [
    "plt.plot(epoch_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QF2wyJM8lN1A"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def imshows(ims):\n",
    "    \"\"\"Visualises a list of dictionaries.\n",
    "\n",
    "    Each key of the dictionary will be used as a column, and\n",
    "    each element of the list will be a row.\n",
    "    \"\"\"\n",
    "    nrow = len(ims)\n",
    "    ncol = len(ims[0])\n",
    "    fig, axes = plt.subplots(nrow, ncol, figsize=(\n",
    "        ncol * 3, nrow * 3), facecolor='white')\n",
    "    for i, im_dict in enumerate(ims):\n",
    "        for j, (title, im) in enumerate(im_dict.items()):\n",
    "            if isinstance(im, torch.Tensor):\n",
    "                im = im.detach().cpu().numpy()\n",
    "            # If RGB, put to end. Else, average across channel dim\n",
    "            if im.ndim > 2:\n",
    "                im = np.moveaxis(im, 0, -1) if im.shape[0] == 3 else np.mean(im, axis=0)\n",
    "\n",
    "            ax = axes[j] if len(ims) == 1 else axes[i, j]\n",
    "            ax.set_title(f\"{title}\\n{im.shape}\")\n",
    "            im_show = ax.imshow(im, cmap='gray')\n",
    "            ax.axis(\"off\")\n",
    "            \n",
    "def inference_gan(model, num_data=4):\n",
    "    to_imshow = []\n",
    "    with torch.no_grad():    \n",
    "        for idx in np.random.choice(len(val_ds), size=num_data, replace=False):\n",
    "            rand_data = val_ds[idx]\n",
    "            rand_input, rand_output_gt = rand_data[\"input\"], rand_data[\"output\"]\n",
    "            rand_output = model(rand_input.to(device)[None])[0]\n",
    "            to_imshow.append(\n",
    "                {\n",
    "                    \"FLAIR\": rand_input[0],\n",
    "                    \"T1w\": rand_input[1],\n",
    "                    \"T2w\": rand_input[2],\n",
    "                    \"GT GD\": rand_output_gt,\n",
    "                    \"inferred GD\": rand_output,\n",
    "                }\n",
    "            )\n",
    "    imshows(to_imshow)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 877
    },
    "id": "P13Mbg09lQIv",
    "outputId": "1bafd579-ba5a-4684-cad8-448d554e221b"
   },
   "outputs": [],
   "source": [
    " inference_gan(model, num_data=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluate from pretrained model\n",
    "\n",
    "in `saved` directory, we have pretrained model with 100 epoches with same configuration\n",
    "with `inference_epoch` function, load checkpoint and inference it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_gan_epoch(work_dir, model,  prefix='ae1', epoch=10, num_data=2):\n",
    "    import torch\n",
    "    model.load_state_dict(torch.load(os.path.join(work_dir, \"gan_{}_model_{:04d}.pth\".format(prefix, epoch))))\n",
    "    model.eval()\n",
    "    inference_gan(model,   num_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- model 1. auto encoder \n",
    "- directory : `saved`\n",
    "- prefix : `ae1`\n",
    "- model : `monai AutoEncoder`\n",
    "- channels : `(4, 8, 16, 32)`\n",
    "- stride  : `(2, 2, 2, 2)`\n",
    "- epochs : 100,200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoEncoder(\n",
    "    spatial_dims =2,\n",
    "    in_channels=in_channels,\n",
    "    out_channels=out_channels,\n",
    "    channels=(4, 8, 16, 32),\n",
    "    strides=(2, 2, 2, 2),\n",
    ").to(device)  # TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_gan_epoch('temp', model,  'ae1', 10, 2)\n",
    "inference_gan_epoch('temp', model,  'ae1', 20, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_gan_epoch('saved', model,  'ae1', 200, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "prBkOMGj28vT"
   },
   "source": [
    "### Model 2. Auto Encoder with more channels\n",
    "\n",
    "<img align='left' src=\"https://miro.medium.com/max/1400/1*nGFy96r63GwSE_EsJDLMDw.png\" width=600>\n",
    "\n",
    " - 4 layers with  `[8, 16, 32, 64]` channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_N2ZjDV53DRz"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    " \n",
    "from monai.networks.nets import AutoEncoder\n",
    "\n",
    "# 4 layers \n",
    "model = AutoEncoder(\n",
    "    spatial_dims =2,\n",
    "    in_channels=in_channels,\n",
    "    out_channels=out_channels,\n",
    "    channels=(8, 16, 32, 64),\n",
    "    strides=(2, 2, 2, 2),\n",
    ").to(device)  # TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i83jXn8JKXC0"
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create loss fn and optimiser\n",
    "\n",
    "loss_function = torch.nn.MSELoss() # TODO\n",
    "\n",
    "learning_rate = 2e-3\n",
    "optimizer = torch.optim.Adam(model.parameters(), learning_rate)  # TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 20  #None  # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0r6PeB853vY1",
    "outputId": "b63bf4ee-029c-403d-95ac-b3a565237773"
   },
   "outputs": [],
   "source": [
    "\n",
    "epoch_losses = []\n",
    "\n",
    "t = trange(max_epochs, desc=f\"epoch 0, avg loss: inf\", leave=True)\n",
    "for epoch in t:\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    for batch in train_dl:\n",
    "        step += 1\n",
    "        inputs, outputs_gt = batch[\"input\"].to(device), batch[\"output\"].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, outputs_gt)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    epoch_loss /= step\n",
    "    epoch_losses.append(epoch_loss)\n",
    "    t.set_description(f\"epoch {epoch + 1}, average loss: {epoch_loss:.4f}\")\n",
    "    if (epoch+1) % 10 == 0: \n",
    "        torch.save(model.state_dict(), os.path.join(root_dir, \"gan_ae2_model_{:04d}.pth\".format(epoch+1)))\n",
    "        torch.save(optimizer.state_dict(), os.path.join(root_dir, \"gan_ae2_optim_{:04d}.pth\".format(epoch+1)))\n",
    "        #print(\"|{}ep model saved\".format(epoch+1), end='')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "ChmYqtqD5WFO",
    "outputId": "55033b67-e51e-4a56-d3bb-e1ad1067ad60"
   },
   "outputs": [],
   "source": [
    "plt.plot(epoch_losses);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 877
    },
    "id": "hJqmhK7R5d0-",
    "outputId": "6e8520ec-7bf5-45c4-b06a-3f4d0abc6f32"
   },
   "outputs": [],
   "source": [
    "inference_gan(model,   num_data=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### inference with pretrained model\n",
    "\n",
    "- model 2. auto encoder \n",
    "- directory : `saved`\n",
    "- prefix : `ae2`\n",
    "- model : `monai AutoEncoder`\n",
    "- channels : `(8, 16, 32, 64)`\n",
    "- stride  : `(2, 2, 2, 2)`\n",
    "- epochs : 100, 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoEncoder(\n",
    "    spatial_dims =2,\n",
    "    in_channels=in_channels,\n",
    "    out_channels=out_channels,\n",
    "    channels=(8, 16, 32, 64),\n",
    "    strides=(2, 2, 2, 2),\n",
    ").to(device)  # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_gan_epoch('temp', model,  'ae2', 10, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_gan_epoch('temp', model,  'ae2', 20, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_gan_epoch('saved', model,  'ae2', 200 , 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_TVlcbjQ4m3g"
   },
   "source": [
    "### Model 3. UNet\n",
    "\n",
    "[U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597)\n",
    "U Shaped Network was developed by Olaf Ronneberger et al. for Bio Medical Image Segmentation. \n",
    "It is Fully Convolutional Network Model for the segmentation task with two paths(encoder and decoder) with 1x1 convolution skip connection similar as residual. \n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*J3t2b65ufsl1x6caf6GiBA.png\" width=800>\n",
    "\n",
    "#### UNet in monai.networks\n",
    "class monai.networks.nets.<b>UNet </b> (`spatial_dims, in_channels, out_channels, channels, strides, kernel_size=3, up_kernel_size=3, num_res_units=0, act='PRELU', norm='INSTANCE', dropout=0.0, bias=True, dimensions=None` )\n",
    "Enhanced version of <br>UNet</b> which has residual units implemented with the `ResidualUnit` class. The residual part uses a convolution to change the input dimensions to match the output dimensions if this is necessary but will use `nn.Identity` if not. Refer to: [Link](https://link.springer.com/chapter/10.1007/978-3-030-12029-0_40).\n",
    "\n",
    "Each layer of the network has a encode and decode path with a skip connection between them. Data in the encode path is downsampled using strided convolutions (if strides is given values greater than 1) and in the decode path upsampled using strided transpose convolutions. These down or up sampling operations occur at the beginning of each block rather than afterwards as is typical in <b>UNet</b> implementations.\n",
    "\n",
    "To further explain this consider the first example network given below. This network has 3 layers with strides of 2 for each of the middle layers (the last layer is the bottom connection which does not down/up sample). Input data to this network is immediately reduced in the spatial dimensions by a factor of 2 by the first convolution of the residual unit defining the first layer of the encode part. The last layer of the decode part will upsample its input (data from the previous layer concatenated with data from the skip connection) in the first convolution. this ensures the final output of the network has the same shape as the input.\n",
    "\n",
    "Padding values for the convolutions are chosen to ensure output sizes are even divisors/multiples of the input sizes if the strides value for a layer is a factor of the input sizes. A typical case is to use strides values of 2 and inputs that are multiples of powers of 2. An input can thus be downsampled evenly however many times its dimensions can be divided by 2, so for the example network inputs would have to have dimensions that are multiples of 4. In the second example network given below the input to the bottom layer will have shape `(1, 64, 15, 15)` for an input of shape `(1, 1, 240, 240)` demonstrating the input being reduced in size spatially by 2**4.\n",
    "\n",
    "##### Parameters\n",
    " - <b>spatial_dims </b>(`int`)  – number of spatial dimensions.\n",
    " - <b>in_channels </b>(`int`) – number of input channels.\n",
    " - <b>out_channels </b>(`int`) – number of output channels.\n",
    " - <b>channels</b> (`Sequence[int]`) – sequence of channels. Top block first. The length of channels should be no less than 2.\n",
    " - <b>strides</b> (`Sequence[int]`) – sequence of convolution strides. The length of stride should equal to len(channels) - 1.\n",
    " - <b>kernel_size</b> (`Union[Sequence[int], int]`) – convolution kernel size, the value(s) should be odd. If sequence, its length should equal to dimensions. Defaults to 3.\n",
    " - <b>up_kernel_size</b> (`Union[Sequence[int], int]`) – upsampling convolution kernel size, the value(s) should be odd. If sequence, its length should equal to dimensions. Defaults to 3.\n",
    " - <b>num_res_units</b>  (`int`)– number of residual units. Defaults to 0.\n",
    " - <b>act</b> (`Union[Tuple, str]`) – activation type and arguments. Defaults to PReLU.\n",
    " - <b>norm</b> (`Union[Tuple, str]`) – feature normalization type and arguments. Defaults to instance norm.\n",
    " - <b>dropout</b> (`float`) – dropout ratio. Defaults to no dropout.\n",
    " - <b>bias (`bool`)</b> – whether to have a bias term in convolution blocks. Defaults to True. According to Performance Tuning Guide, if a conv layer is directly followed by a batch norm layer, bias should be False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gspj_zgP4nFz"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "from monai.networks.nets import UNet\n",
    "\n",
    "# 4 layers \n",
    "model = UNet(\n",
    "    spatial_dims =2,\n",
    "    in_channels=in_channels,\n",
    "    out_channels=out_channels,\n",
    "    channels=(4, 8, 16, 32, 64),\n",
    "    strides=(2, 2, 2, 2),\n",
    ").to(device)  # TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create loss fn and optimiser\n",
    "\n",
    "loss_function = torch.nn.MSELoss() # TODO\n",
    "\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.Adam(model.parameters(), learning_rate)  # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 20  #None  # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BfCW2PQVKiE7"
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YsNCA7JQ4nO6",
    "outputId": "a697a0a4-a3e5-433b-f530-392e8b40257d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "epoch_losses = []\n",
    "\n",
    "t = trange(max_epochs, desc=f\"epoch 0, avg loss: inf\", leave=True)\n",
    "for epoch in t:\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    for batch in train_dl:\n",
    "        step += 1\n",
    "        inputs, outputs_gt = batch[\"input\"].to(device), batch[\"output\"].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, outputs_gt)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    epoch_loss /= step\n",
    "    epoch_losses.append(epoch_loss)\n",
    "    \n",
    "    if (epoch+1) % 10 == 0: \n",
    "        torch.save(model.state_dict(), os.path.join(root_dir, \"gan_unet_model_{:04d}.pth\".format(epoch+1)))\n",
    "        torch.save(optimizer.state_dict(), os.path.join(root_dir, \"gan_unet_optim_{:04d}.pth\".format(epoch+1)))\n",
    "        #print(\"|{}ep model saved\".format(epoch+1), end='')\n",
    "        \n",
    "    t.set_description(f\"epoch {epoch + 1}, average loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "cTi4D5Eo6c8P",
    "outputId": "92b7a2c6-a5f4-49bf-bef9-87d5cc847ca3"
   },
   "outputs": [],
   "source": [
    "plt.plot(epoch_losses);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 877
    },
    "id": "MgVNdUen6iET",
    "outputId": "506c5763-f37b-41d6-f309-4cc94ca28ca6"
   },
   "outputs": [],
   "source": [
    "inference_gan(model,   num_data=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inference with pre-trained model\n",
    "\n",
    "\n",
    "\n",
    "- model 3. Unet  \n",
    "- directory : `saved`\n",
    "- prefix : `unet`\n",
    "- model : `monai UNet`\n",
    "- channels : `(4, 8, 16, 32, 64)`\n",
    "- stride  : `(2, 2, 2, 2)`\n",
    "- epochs : 200,1000\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(\n",
    "    spatial_dims =2,\n",
    "    in_channels=in_channels,\n",
    "    out_channels=out_channels,\n",
    "    channels=(4, 8, 16, 32, 64),\n",
    "    strides=(2, 2, 2, 2),\n",
    ").to(device)  # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_gan_epoch('temp', model,  'unet', 10, 2)\n",
    "inference_gan_epoch('temp', model,  'unet', 20, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_gan_epoch('saved', model,  'unet', 200, 2)\n",
    "inference_gan_epoch('saved', model,  'unet', 1000, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# enable AMP (Optinal)\n",
    "Deep Neural Network training has traditionally relied on IEEE single-precision format, however with mixed precision, you can train with half precision while maintaining the network accuracy achieved with single precision. This technique of using both single- and half-precision representations is referred to as mixed precision technique. Mixed precision methods combine the use of different numerical formats in one computational workload. \n",
    "\n",
    "### Benefits of Mixed precision training\n",
    "- Speeds up math-intensive operations, such as linear and convolution layers, by using Tensor Cores.\n",
    "- Speeds up memory-limited operations by accessing half the bytes compared to single-precision.\n",
    "- Reduces memory requirements for training models, enabling larger models or larger minibatches.\n",
    "\n",
    "Enabling mixed precision involves two steps: porting the model to use the half-precision data type where appropriate, and using loss scaling to preserve small gradient values. Deep learning researchers and engineers can easily get started enabling this feature on Ampere, Volta and Turing GPUs.\n",
    "\n",
    "On Ampere GPUs, automatic mixed precision uses FP16 to deliver a performance boost of 3X versus TF32, the new format which is already ~6x faster than FP32. On Volta and Turing GPUs, automatic mixed precision delivers up to 3X higher performance vs FP32 with just a few lines of code. The best training performance on NVIDIA GPUs is always available on the NVIDIA deep learning performance page.[link1](https://developer.nvidia.com/deep-learning-performance-training-inference) \n",
    "[link2](https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html) \n",
    "\n",
    "![](https://developer.nvidia.com/sites/default/files/dev-ai-tech-amp-workflow-graphic-950353-r7-web.png)\n",
    "![precision](https://developer-blogs.nvidia.com/wp-content/uploads/2021/01/AI_training_TF32_tensor_cores_F2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Create loss fn and optimiser\n",
    "max_epochs = 10  #None  # TODO\n",
    "\n",
    "from monai.networks.nets import UNet\n",
    "\n",
    "# 4 layers \n",
    "model = UNet(\n",
    "    spatial_dims =2,\n",
    "    in_channels=in_channels,\n",
    "    out_channels=out_channels,\n",
    "    channels=(4, 8, 16, 32, 64),\n",
    "    strides=(2, 2, 2, 2),\n",
    ").to(device)  # TODO\n",
    "\n",
    "loss_function = torch.nn.MSELoss() # TODO\n",
    "\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.Adam(model.parameters(), learning_rate)  # TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with `with autocast(enabled=amp)`, we can enable AMP(Automatic Mixed Precision) with FP16.\n",
    "pytorch also need scaler for AMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### module load for AMP \n",
    "from torch.cuda.amp import GradScaler, autocast  ## for AMP \n",
    "\n",
    "###### configuration for AMP \n",
    "amp = True \n",
    "\n",
    "###### scaler for AMP \n",
    "if amp is True:\n",
    "    scaler = GradScaler()\n",
    "else:\n",
    "    scaler = None\n",
    "\n",
    "\n",
    "epoch_losses = []\n",
    "\n",
    "t = trange(max_epochs, desc=f\"epoch 0, avg loss: inf\", leave=True)\n",
    "for epoch in t:\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    for batch in train_dl:\n",
    "        step += 1\n",
    "        inputs, outputs_gt = batch[\"input\"].to(device), batch[\"output\"].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        ###### autocast  for AMP in native pytorch\n",
    "        with autocast(enabled=amp):\n",
    "            outputs = model(inputs)        \n",
    "            loss = loss_function(outputs, outputs_gt)\n",
    "            \n",
    "        ###### backward with AMP in native pytorch            \n",
    "        if amp:    \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()            \n",
    "        else :\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    epoch_loss /= step\n",
    "    epoch_losses.append(epoch_loss)\n",
    "    t.set_description(f\"epoch {epoch + 1}, average loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epoch_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.eval()\n",
    "\n",
    "for idx in np.random.choice(len(val_ds), size=5, replace=False):\n",
    "    rand_data = val_ds[idx]\n",
    "    rand_input, rand_output_gt = rand_data[\"input\"], rand_data[\"output\"]\n",
    "    rand_output = model(rand_input.to(device)[None])[0]\n",
    "        \n",
    "    to_imshow.append(\n",
    "        {\n",
    "            \"FLAIR\": rand_input[0],\n",
    "            \"T1w\": rand_input[1],\n",
    "            \"T2w\": rand_input[2],\n",
    "            \"GT GD\": rand_output_gt,\n",
    "            \"inferred GD\": rand_output,\n",
    "        }\n",
    "    )\n",
    "imshows(to_imshow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caution !!!\n",
    "### please shutdown all kernels with [Kernel] menu >  [Shutdown All Kernel]  before launch next notebook\n",
    "\n",
    "## Navigation\n",
    "- [01_getting started](./01_getting.ipynb)\n",
    "\n",
    "- [02_pipeline_01](./02_pipeline_01.ipynb)\n",
    "- [02_pipeline_02 ](./02_pipeline_02.ipynb)\n",
    "- [02_pipeline_03](./02_pipeline_03.ipynb)\n",
    "- [02_pipeline_04  ](./02_pipeline_04.ipynb)\n",
    "\n",
    "- [03_brain_gan ](./03_brain_gan_01.ipynb)\n",
    "\n",
    "- [04_spleen_segment Next](./04_spleen_segment.ipynb) \n",
    "\n",
    "- [05_challenge_cardiac baseline](./05_challenge_cardiac_baseline.ipynb) \n",
    "\n",
    "- [05_challenge_cardiac workspace](./05_challenge_cardiac_workspace.ipynb) \n",
    "\n",
    "<img src=\"https://github.com/Project-MONAI/MONAIBootcamp2021/raw/2f28b64f814a03703667c8ea18cc84f53d6795e4/day1/monai.png\" width=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "moani_bootcamp_2_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
